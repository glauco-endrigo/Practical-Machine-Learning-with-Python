{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Practical Machine Learning with Python from  the youtube channel sentdex <br />\n",
    "\n",
    "\"The goal is to break it down so much that it is panfully simple \"\n",
    "\n",
    "\n",
    "#### Episode:\n",
    "\n",
    "\n",
    "* [Creating an SVM from scratch - Practical Machine Learning Tutorial with Python p.25:](https://www.youtube.com/watch?v=AbVtcUBlBok&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective: \n",
    "\n",
    "Learn about Practical Machine Learning with Python from Scratch with Python .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction: \n",
    "The purpuse is to know how Machine Learning works at a very deep level, because trying to solve more complex problems is gonna require a deep understand of how things actually work.\n",
    "We are gonna be doing this buy covering a variety of algorithms, so first we are gonna be covering regression when we are going to be movingitno classification with knearst-neighbors and support vector machines and then we are going to get into clustering with flat clustering, hierarchical clustering and then finally we will be getting  into deep learning with neural networks \n",
    "\n",
    "In each of the major algorithms, we are gonna cover theory, aplicattion and then we are gonna dive in deep into the inner workings of each of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Support Vector Machine\n",
    "\n",
    "\n",
    "The support vector Machine is what is called a binary classifier, so it separetes only into \n",
    "to groups at a time. These two groups are denoted as positive or negative.The objective of \n",
    "the svm is to find the best separeting hyperplane which is also refered  to as your decision boundary(that is why ideally we want it to be agaist linear data). And then we can classify new data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import style \n",
    "import numpy as np\n",
    "sule.use('ggplot')\n",
    "\n",
    "data_dict = {-1:np.array([[1,7]\n",
    "                         ,[2,8]\n",
    "                         ,[3,8]])\n",
    "             \n",
    "             ,1:np.array([[5,1]\n",
    "                         ,[6,-1]\n",
    "                         ,[7,3]]) \n",
    "            }\n",
    "\n",
    "class Support_Vector_Machine:\n",
    "    def __int__(self, visualization=True):\n",
    "        self.visualization = visualization\n",
    "        self.colors = {1:'r', -1:'b'}\n",
    "        if self.visualization:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax  = self.fig.add_subplot(1,1,1)\n",
    "    \n",
    "    def fit(self,data)\n",
    "        self.data = data   \n",
    "        opt_dict = {}\n",
    "        transforms = [ [1,1]\n",
    "                      ,[-1,1]\n",
    "                      ,[-1,-1]\n",
    "                      ,[1,-1]   ]\n",
    "        all_data = []\n",
    "    \n",
    "        for yi in self.data:\n",
    "            for featureset in self.data[yi]:\n",
    "                for feaure in featureset:\n",
    "                    all_data.append(feaure)\n",
    "        self.max_feature_value = max(all_data)\n",
    "        self.min_feature_value = min(all_data) \n",
    "        all_data = None \n",
    "        \n",
    "        step_sizes = [self.max_feature_value*0.1\n",
    "                     ,self.max_feature_value*0.01\n",
    "                     ,self.max_feature_value*0.001] # point of expense\n",
    "        b_range_multiple  = 5 # extremely expensive\n",
    "        b_multiple        = 5\n",
    "        latest_optimum = self.max_feature_value*10\n",
    "        \n",
    "        for step in step_sizes:\n",
    "            w = np.array([latest_optimum,latest_optimum]) # major cornes being cut \n",
    "        \n",
    "        optimized = False   # we can do this because  convex\n",
    "        while not optmized:\n",
    "            pass  \n",
    "            \n",
    "    def predict (self, features)\n",
    "        classification = np.sign(np.dot(np.array(features),self.w + self.b))\n",
    "        return classification\n",
    "            \n",
    "    \n",
    "            \n",
    "          \n",
    "        \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
